{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 作业2：纯天然手工制作神经网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本次作业中，你将体验到如何用最基本的矩阵运算来实现一个前馈神经网络。在该实现中，唯一需要使用到的软件包是Numpy。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 第1题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "利用 Git 的基本操作，对第一次作业进行一次修订。具体要求如下：\n",
    "\n",
    "1. 将库的名称统一为 “ai2021s”。\n",
    "2. 在库的根目录创建文件夹 `HW1`，将第一次作业提交的 Notebook 文件重命名为 `numpy_practice.ipynb`，并移至 `HW1` 文件夹中。\n",
    "3. 提交这一修改，提交信息为 “reorganizing project files”。\n",
    "4. 从本次作业起，均按上述规则建立文件夹。本次作业的文件名为`fnn.ipynb`。\n",
    "5. 优化作业1中各函数的编写，直接对原始文件修改并利用 Git 命令提交。根据本次修改的结果，你将对作业1获得最多20分的加分。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 第2题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) 根据作业1的内容，编写**数值稳定**的函数 `sigmoid(x)` 和 `softplus(x)`，要求它适用于 Numpy 向量和矩阵，即当 `x` 是一维或二维数组时，返回一个相同大小和形状的数组。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Sigmoid(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n",
      "Softplus(v) =\n",
      " [0.29094333 1.31132175 0.84461281 0.20026791 0.44512331 1.82687967]\n",
      "Softplus(m) =\n",
      " [[0.08464411 0.50151249 1.51433825]\n",
      " [0.35088177 0.41024142 0.6469135 ]\n",
      " [1.69437919 0.42387573 0.49559644]\n",
      " [0.49937109 2.31042345 2.29319537]\n",
      " [1.31622694 0.90476816 1.12830942]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "def sigmoid(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = 1 / (1 + np.exp(-x))\n",
    "    return res\n",
    "\n",
    "def softplus(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = np.log(1 + np.exp(-abs(x))) + np.maximum(x,0)\n",
    "    return res\n",
    "\n",
    "np.random.seed(123)\n",
    "vec = np.random.normal(size=6)\n",
    "mat = np.random.normal(size=(5, 3))\n",
    "print(\"Sigmoid(v) =\\n\", sigmoid(vec))\n",
    "print(\"Sigmoid(m) =\\n\", sigmoid(mat))\n",
    "print(\"Softplus(v) =\\n\", softplus(vec))\n",
    "print(\"Softplus(m) =\\n\", softplus(mat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) 推导 Sigmoid 和 Softplus 函数的导数，编写**数值稳定**的函数 `d_sigmoid(x)` 和 `d_softplus(x)` 计算对应的导数，同样要求适用于向量和矩阵。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\sigma(x)=1/(1+\\exp(-x)),\\ \\sigma'(x)=...$\n",
    "\n",
    "$\\mathrm{softplus}(x)=\\log(1+\\exp(x)),\\ \\mathrm{softplus}'(x)=...$\n",
    "\n",
    "注意到 Sigmoid 和 Softplus 之间的联系了吗？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid'(v) =\n",
      " [0.18871502 0.196853   0.24506124 0.14855047 0.23019077 0.13502129]\n",
      "Sigmoid'(m) =\n",
      " [[0.07457369 0.23884569 0.17157406]\n",
      " [0.20835666 0.223271   0.24944023]\n",
      " [0.14996269 0.22612814 0.23807373]\n",
      " [0.23856977 0.08937477 0.09075383]\n",
      " [0.19624332 0.24090565 0.21887592]]\n",
      "Softplus'(v) =\n",
      " [0.25244196 0.73053634 0.57027629 0.18148857 0.35925474 0.83908511]\n",
      "Softplus'(m) =\n",
      " [[0.08116076 0.39438602 0.78004631]\n",
      " [0.29593301 0.33650995 0.47634044]\n",
      " [0.81628676 0.34549479 0.39079256]\n",
      " [0.39308777 0.90078077 0.89905661]\n",
      " [0.73185488 0.59536432 0.67642017]]\n"
     ]
    }
   ],
   "source": [
    "def d_sigmoid(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return res\n",
    "\n",
    "def d_softplus(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = sigmoid(x)\n",
    "    return res\n",
    "\n",
    "print(\"Sigmoid'(v) =\\n\", d_sigmoid(vec))\n",
    "print(\"Sigmoid'(m) =\\n\", d_sigmoid(mat))\n",
    "print(\"Softplus'(v) =\\n\", d_softplus(vec))\n",
    "print(\"Softplus'(m) =\\n\", d_softplus(mat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) 编写函数 `relu(x)` 和 `d_relu(x)` 来实现 ReLU 函数及其导数，$\\mathrm{ReLU}(x)=\\max(x,0)$，要求适用于向量和矩阵。对于不可导的点，可以将导数取为左导数或右导数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(v) =\n",
      " [0.         0.99734545 0.2829785  0.         0.         1.65143654]\n",
      "ReLU(m) =\n",
      " [[0.         0.         1.26593626]\n",
      " [0.         0.         0.        ]\n",
      " [1.49138963 0.         0.        ]\n",
      " [0.         2.20593008 2.18678609]\n",
      " [1.0040539  0.3861864  0.73736858]]\n",
      "ReLU'(v) =\n",
      " [0 1 1 0 0 1]\n",
      "ReLU'(m) =\n",
      " [[0 0 1]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = np.maximum(x,0)\n",
    "    return res\n",
    "\n",
    "def d_relu(x):\n",
    "    res = np.zeros_like(x)\n",
    "    res = np.where(x>0,1,0)\n",
    "    return res\n",
    "\n",
    "print(\"ReLU(v) =\\n\", relu(vec))\n",
    "print(\"ReLU(m) =\\n\", relu(mat))\n",
    "print(\"ReLU'(v) =\\n\", d_relu(vec))\n",
    "print(\"ReLU'(m) =\\n\", d_relu(mat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 第3题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "练习标量的反向传播算法。考虑一个两层的前馈神经网络（见第5周课件107页），其中数据和权重都是标量：\n",
    "\n",
    "![](img/model1.png)\n",
    "\n",
    "如下为正向的计算过程："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "x = 1.2345\n",
    "w1 = 0.111\n",
    "b1 = 0.222\n",
    "w2 = 0.333\n",
    "b2 = -0.444\n",
    "\n",
    "a0 = x\n",
    "z1 = w1 * a0 + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = w2 * a1 + b2\n",
    "a2 = z2\n",
    "\n",
    "yhat = a2\n",
    "y = 5.4321"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "令损失函数为 $l=(y-\\hat{y})^2$，则可以计算出 $l$ 对 $a_2=\\hat{y}$ 的导数为 $\\mathrm{d}l/\\mathrm{d}a_2=2(a_2-y)$："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "d_l_a2 = 2.0 * (a2 - y) # This is the \"upstream derivative\" associated with a2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在将 $a_2$ 的导数反向传播给 $z_2$。首先计算 $a_2$ 对 $z_2$ 的“局部导数”：因为 $a_2=z_2$，所以局部导数为1。然后将“上游导数”乘以“局部导数”，即得到“下游导数”。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_a2_z2 = 1.0 # The \"local derivative\" of a2 with respect to z2\n",
    "d_l_z2 = d_l_a2 * d_a2_z2 # The \"downstream derivative\" of l with respect to z2\n",
    "\n",
    "print(d_l_z2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来将 $z_2$ 的导数继续向下游传播。在上一层的计算中得到的“下游导数”现在变成了“上游导数”，进而向 $a_1$、$w_2$ 和 $b_2$ 传播。依然进行局部导数的计算，可以得到 $\\mathrm{d}z_2/\\mathrm{d}a_1=w_2$, $\\mathrm{d}z_2/\\mathrm{d}w_2=a_1$, $\\mathrm{d}z_2/\\mathrm{d}b_2=1$。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "d_z2_a1 = w2\n",
    "d_z2_w2 = a1\n",
    "d_z2_b2 = 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "于是通过链式法则，将“上游导数”乘以“局部导数”得到“下游导数”。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.7828984975710127\n",
      "-6.688862995036224\n",
      "-11.360055548261299\n"
     ]
    }
   ],
   "source": [
    "d_l_a1 = d_l_z2 * d_z2_a1\n",
    "d_l_w2 = d_l_z2 * d_z2_w2\n",
    "d_l_b2 = d_l_z2 * d_z2_b2\n",
    "\n",
    "print(d_l_a1)\n",
    "print(d_l_w2)\n",
    "print(d_l_b2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}l/\\mathrm{d}w_1$ 和 $\\mathrm{d}l/\\mathrm{d}b_1$。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "d_a1_z1 = d_sigmoid(z1)\n",
    "d_z1_w1 = x\n",
    "d_z1_b1 = 1.0\n",
    "d_l_z1 = d_l_a1 * d_a1_z1\n",
    "\n",
    "d_l_w1 = d_l_z1 * d_z1_w1\n",
    "d_l_b1 = d_l_z1 * d_z1_b1\n",
    "\n",
    "print(d_l_w1)\n",
    "print(d_l_b1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1306675893376559\n",
      "-0.9158911213751769\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 第4题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在考虑一个更实际的场景，首先，数据包含多个观测，每个观测占据一行："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看出输入的维度为 $p=2$，输出的维度为 $d=1$，样本量为 $n=4$。我们将构建一个两层的前馈神经网络，其中隐藏层的维度为 $r=5$。计算流程如下：\n",
    "\n",
    "![](img/model2.png)\n",
    "\n",
    "$Z_1=XW_1+\\mathbf{1}_nb_1^T,\\quad A_1=\\mathrm{softplus}(Z_1)$\n",
    "\n",
    "$Z_2=A_1W_2+\\mathbf{1}_nb_2^T,\\quad A_2=\\mathrm{sigmoid}(Z_2)$\n",
    "\n",
    "其中 $\\mathbf{1}_n$ 为元素全为1的 $n\\times 1$ 向量，$W_1$ 为 $p\\times r$ 矩阵，$b_1$ 为 $r\\times 1$ 向量，$W_2$ 为 $r\\times d$ 矩阵，$b_2$ 为 $d\\times 1$ 向量。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "a0 = x\n",
    "ones = np.ones((n, 1))\n",
    "z1 = a0.dot(w1) + ones.dot(b1.T)\n",
    "a1 = softplus(z1)\n",
    "z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "phat = a2\n",
    "print(phat)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义第 $i$ 个观测的损失函数为 $l(y_i,\\hat{p}_i)=-y_i \\cdot \\log(\\hat{p}_i)-(1-y_i) \\cdot \\log(1-\\hat{p}_i)$，请推导出 $l$ 对 $\\hat{p}_i$ 的导数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\mathrm{d}l/\\mathrm{d}\\hat{p}_i=-y_i/\\hat{p}_i + (1-y_i)/(1-\\hat{p}_i)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义整体的损失函数为 $L(y,\\hat{p})=n^{-1}\\sum_{i=1}^n l(y_i,p_i)$，请给出 $L$ 对 $\\hat{p}_i$ 的导数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\mathrm{d}L/\\mathrm{d}\\hat{p}_i=\\frac{\\hat{p}_i-y_i}{n\\hat{p}_i(1-\\hat{p}_i)}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将 $\\mathrm{d}L/\\mathrm{d}\\hat{p}=(\\mathrm{d}L/\\mathrm{d}\\hat{p}_1,\\ldots,\\mathrm{d}L/\\mathrm{d}\\hat{p}_n)^T$ 看作一个向量，计算出其结果并赋给变量 `d_l_a2`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "d_l_a2 = (a2 - y)/(len(a2) * a2 * (ones - a2))\n",
    "\n",
    "print(d_l_a2)\n",
    "\n",
    "if d_l_a2.shape != a2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137.54924583]\n",
      " [ -0.25286142]\n",
      " [ -0.25009615]\n",
      " [ 29.03650883]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`d_l_a2` 是损失函数对 $A_2$ 的“上游导数”，我们需要将其传播给下游 $Z_2$。由于 $A_2=\\mathrm{sigmoid}(Z_2)$，根据课件中反向传播的规则1，可以得到下游导数为\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}Z_2=\\mathrm{d}L/\\mathrm{d}A_2\\circ \\mathrm{sigmoid}'(Z_2)$，\n",
    "\n",
    "其中 $\\mathrm{sigmoid}'$ 是 Sigmoid 的导数，$\\circ$ 是向量或矩阵的逐元素相乘。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.49545617e-01]\n",
      " [-2.82903496e-03]\n",
      " [-9.61178724e-05]\n",
      " [ 2.47847537e-01]]\n"
     ]
    }
   ],
   "source": [
    "d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "print(d_l_z2)\n",
    "\n",
    "if d_l_z2.shape != z2.shape:\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，再将 $Z_2$ 的“上游导数”传给 $A_1$、$W_2$ 和 $b_2$。根据规则2，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}A_1=(\\mathrm{d}L/\\mathrm{d}Z_2)W_2^T$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}W_2=A_1^T(\\mathrm{d}L/\\mathrm{d}Z_2)$，\n",
    "\n",
    "$\\mathrm{d}L/\\mathrm{d}b_2=(\\mathrm{d}L/\\mathrm{d}Z_2)^T\\mathbf{1}_n$。\n",
    "\n",
    "请根据如上公式计算对应的导数，并赋值给相应的变量。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "d_l_a1 = d_l_z2.dot(w2.T)\n",
    "print(\"d_A1:\\n\", d_l_a1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_A1:\n",
      " [[-1.08390457e-01  5.50480184e-01  5.45702884e-01  2.50557250e-01  9.63711234e-02]\n",
      " [ 1.22879494e-03 -6.24065331e-03 -6.18649429e-03 -2.84050357e-03 -1.09253482e-03]\n",
      " [ 4.17489205e-05 -2.12029306e-04 -2.10189226e-04 -9.65075245e-05 -3.71194151e-05]\n",
      " [-1.07652894e-01  5.46734339e-01  5.41989547e-01  2.48852286e-01  9.57153480e-02]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_W2:\n",
      " [[0.25687749]\n",
      " [0.20993967]\n",
      " [0.80962486]\n",
      " [0.18883191]\n",
      " [0.15786162]]\n"
     ]
    }
   ],
   "source": [
    "d_l_w2 = a1.T.dot(d_l_z2)\n",
    "print(\"d_W2:\\n\", d_l_w2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_b2:\n",
      " [[0.494468]]\n"
     ]
    }
   ],
   "source": [
    "d_l_b2 = d_l_z2.T.dot(ones)\n",
    "print(\"d_b2:\\n\", d_l_b2)\n",
    "\n",
    "if (d_l_a1.shape != a1.shape) or (d_l_w2.shape != w2.shape) or (d_l_b2.shape != b2.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "请完成后续的过程，计算出 $\\mathrm{d}L/\\mathrm{d}W_1$ 和 $\\mathrm{d}L/\\mathrm{d}b_1$。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "d_l_z1 = d_l_a1 * d_softplus(z1)\n",
    "d_l_w1 = x.T.dot(d_l_z1)\n",
    "print(\"d_W1:\\n\", d_l_w1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_W1:\n",
      " [[-0.05078023  0.09764186  0.42982702  0.07298599  0.01255969]\n",
      " [-0.04989469  0.09732858  0.42540916  0.07114462  0.01233749]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_b1:\n",
      " [[-0.08636305]\n",
      " [ 0.3593937 ]\n",
      " [ 0.87067948]\n",
      " [ 0.15770073]\n",
      " [ 0.04998879]]\n"
     ]
    }
   ],
   "source": [
    "d_l_b1 = d_l_z1.T.dot(ones)\n",
    "print(\"d_b1:\\n\", d_l_b1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "if (d_l_w1.shape != w1.shape) or (d_l_b1.shape != b1.shape):\n",
    "    print(\"Shapes do not match! Please redo the calculation.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 第5题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将第4题中的步骤封装成三个函数：`forward()` 计算正向传播结果，得到预测值向量 `phat`，`loss()` 计算整体损失函数值，`backward()` 计算反向传播后各参数的导数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "def forward(x, w1, b1, w2, b2):\n",
    "    ones = np.ones((len(x), 1))\n",
    "    z1 = x.dot(w1) + ones.dot(b1.T)\n",
    "    a1 = softplus(z1)\n",
    "    z2 = a1.dot(w2) + ones.dot(b2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def loss_function(y, phat):\n",
    "    loss = -y * np.log(phat) - (ones - y) * np.log(ones - phat)\n",
    "    return loss\n",
    "\n",
    "def backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y):\n",
    "    ones = np.ones((len(x), 1))\n",
    "    d_l_a2 = (a2 - y)/(len(a2) * a2 * (ones - a2))\n",
    "    d_l_z2 = d_l_a2 * d_sigmoid(z2)\n",
    "    d_l_a1 = d_l_z2.dot(w2.T)\n",
    "    d_l_w2 = a1.T.dot(d_l_z2)\n",
    "    d_l_b2 = d_l_z2.T.dot(ones)\n",
    "    d_l_z1 = d_l_a1 * d_softplus(z1)\n",
    "    d_l_w1 = x.T.dot(d_l_z1)\n",
    "    d_l_b1 = d_l_z1.T.dot(ones)\n",
    "    return d_l_w1, d_l_b1, d_l_w2, d_l_b2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "与之前的结果互相印证："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phat =\n",
      " [[0.99818247]\n",
      " [0.98868386]\n",
      " [0.99961553]\n",
      " [0.99139015]]\n",
      "loss = [[6.31027637e+00]\n",
      " [1.13806545e-02]\n",
      " [3.84545418e-04]\n",
      " [4.75484832e+00]]\n",
      "(dw1, db1, dw2, db2) =\n",
      " (array([[-0.05078023,  0.09764186,  0.42982702,  0.07298599,  0.01255969],\n",
      "       [-0.04989469,  0.09732858,  0.42540916,  0.07114462,  0.01233749]]), array([[-0.08636305],\n",
      "       [ 0.3593937 ],\n",
      "       [ 0.87067948],\n",
      "       [ 0.15770073],\n",
      "       [ 0.04998879]]), array([[0.25687749],\n",
      "       [0.20993967],\n",
      "       [0.80962486],\n",
      "       [0.18883191],\n",
      "       [0.15786162]]), array([[0.494468]]))\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])\n",
    "p = 2\n",
    "d = 1\n",
    "n = 4\n",
    "r = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "phat = a2\n",
    "loss = loss_function(y, phat)\n",
    "grads = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "\n",
    "print(\"phat =\\n\", phat)\n",
    "print(\"loss =\", loss)\n",
    "print(\"(dw1, db1, dw2, db2) =\\n\", grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后将这三步放在循环中，并利用梯度下降法拟合出 $x$ 和 $y$ 之间的函数关系。下面是一个完整的神经网络模型，我们可以修改隐藏层的维度。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "x = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "y = np.array([[0.0],\n",
    "              [1.0],\n",
    "              [1.0],\n",
    "              [0.0]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = [[5.14913521e-04]\n",
      " [8.03399377e+00]\n",
      " [1.04324720e+01]\n",
      " [9.50109230e-06]], prediction = [5.14780976e-04 3.24250638e-04 2.94601507e-05 9.50104717e-06]\n",
      "iteration 50, loss = [[0.4667064 ]\n",
      " [0.29426678]\n",
      " [0.96291145]\n",
      " [0.56172046]], prediction = [0.37293583 0.74507769 0.38177974 0.42977283]\n",
      "iteration 100, loss = [[0.46951389]\n",
      " [0.33713845]\n",
      " [0.58821662]\n",
      " [0.43905391]], prediction = [0.37469384 0.71381001 0.55531674 0.35535397]\n",
      "iteration 150, loss = [[0.40507747]\n",
      " [0.2814265 ]\n",
      " [0.41335517]\n",
      " [0.31911582]], prediction = [0.33307486 0.75470638 0.66142732 0.27320863]\n",
      "iteration 200, loss = [[0.31361169]\n",
      " [0.21456717]\n",
      " [0.29000958]\n",
      " [0.22979338]], prediction = [0.26919725 0.80689061 0.7482564  0.20530222]\n",
      "iteration 250, loss = [[0.23125947]\n",
      " [0.15786057]\n",
      " [0.20380942]\n",
      " [0.16569784]], prediction = [0.20646646 0.85396884 0.81561779 0.15269778]\n",
      "iteration 300, loss = [[0.16999045]\n",
      " [0.11615697]\n",
      " [0.1460256 ]\n",
      " [0.12134952]], prediction = [0.15632713 0.89033546 0.86413558 0.11427567]\n",
      "iteration 350, loss = [[0.12751662]\n",
      " [0.08723291]\n",
      " [0.10783971]\n",
      " [0.09122077]], prediction = [0.11972121 0.91646361 0.89777149 0.08718384]\n",
      "iteration 400, loss = [[0.09838089]\n",
      " [0.06734549]\n",
      " [0.08230744]\n",
      " [0.07062689]], prediction = [0.09369637 0.93487216 0.92098877 0.06819051]\n",
      "iteration 450, loss = [[0.07807001]\n",
      " [0.05345477]\n",
      " [0.06478908]\n",
      " [0.05624881]], prediction = [0.07510033 0.94794881 0.93726513 0.0546961 ]\n",
      "iteration 500, loss = [[0.06354263]\n",
      " [0.04350672]\n",
      " [0.05239673]\n",
      " [0.04593483]], prediction = [0.06156589 0.95742612 0.94895231 0.04489579]\n",
      "iteration 550, loss = [[0.05286243]\n",
      " [0.03618765]\n",
      " [0.04336162]\n",
      " [0.03832843]], prediction = [0.05148951 0.9644593  0.95756505 0.03760319]\n",
      "iteration 600, loss = [[0.04480306]\n",
      " [0.03066247]\n",
      " [0.03658854]\n",
      " [0.03257144]], prediction = [0.04381423 0.96980286 0.96407274 0.0320467 ]\n",
      "iteration 650, loss = [[0.03857608]\n",
      " [0.02639286]\n",
      " [0.03138381]\n",
      " [0.02811122]], prediction = [0.0378415  0.97395239 0.96910355 0.02771977]\n",
      "iteration 700, loss = [[0.03366306]\n",
      " [0.02302419]\n",
      " [0.02729624]\n",
      " [0.02458345]], prediction = [0.03310276 0.97723884 0.97307294 0.02428373]\n",
      "iteration 750, loss = [[0.02971465]\n",
      " [0.02031719]\n",
      " [0.02402426]\n",
      " [0.02174196]], prediction = [0.02927751 0.97988782 0.97626203 0.02150731]\n",
      "iteration 800, loss = [[0.02648965]\n",
      " [0.0181065 ]\n",
      " [0.02136107]\n",
      " [0.01941637]], prediction = [0.02614188 0.98205644 0.97886546 0.01922909]\n",
      "iteration 850, loss = [[0.0238177 ]\n",
      " [0.01627528]\n",
      " [0.01916142]\n",
      " [0.01748603]], prediction = [0.02353629 0.98385645 0.98102099 0.01733403]\n",
      "iteration 900, loss = [[0.0215759 ]\n",
      " [0.01473921]\n",
      " [0.01732102]\n",
      " [0.0158637 ]], prediction = [0.0213448  0.98536888 0.98282813 0.01573854]\n",
      "iteration 950, loss = [[0.01967388]\n",
      " [0.01343625]\n",
      " [0.01576349]\n",
      " [0.01448513]], prediction = [0.01948162 0.98665361 0.98436011 0.01438072]\n"
     ]
    }
   ],
   "source": [
    "n = x.shape[0]\n",
    "p = x.shape[1]\n",
    "d = y.shape[1]\n",
    "r = 10\n",
    "\n",
    "np.random.seed(123)\n",
    "w1 = np.random.normal(size=(p, r))\n",
    "b1 = np.random.normal(size=(r, 1))\n",
    "w2 = np.random.normal(size=(r, d))\n",
    "b2 = np.random.normal(size=(d, 1))\n",
    "\n",
    "nepoch = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(nepoch):\n",
    "    z1, a1, z2, a2 = forward(x, w1, b1, w2, b2)\n",
    "    phat = a2\n",
    "    loss = loss_function(y, phat)\n",
    "    dw1, db1, dw2, db2 = backward(x, w1, b1, w2, b2, z1, a1, z2, a2, y)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss}, prediction = {phat.squeeze()}\")\n",
    "\n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}